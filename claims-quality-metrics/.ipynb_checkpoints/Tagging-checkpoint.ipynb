{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install ufal.morphodita -U --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.morphodita import *\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_entities(text):\n",
    "    return text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.morphodita import (\n",
    "    Forms, TokenRanges, Tokenizer_newCzechTokenizer, \n",
    "    Tokenizer_newEnglishTokenizer, Tokenizer_newGenericTokenizer, \n",
    "    Tokenizer_newVerticalTokenizer, TaggedLemmasForms\n",
    ")\n",
    "\n",
    "class MorphoDiTaTokenizer:\n",
    "    def __init__(self, lang:str =\"cs\"):\n",
    "        lang = lang.lower()\n",
    "        assert lang in [\"cs\", \"en\", \"generic\", \"vertical\"]\n",
    "        if lang == \"cs\":\n",
    "            self.tokenizer = Tokenizer_newCzechTokenizer()\n",
    "        elif lang == \"en\":\n",
    "            self.tokenizer = Tokenizer_newCzechTokenizer()\n",
    "        elif lang == \"generic\":\n",
    "            self.tokenizer = Tokenizer_newGenericTokenizer()\n",
    "        elif lang == \"vertical\":\n",
    "            self.tokenizer = Tokenizer_newVerticalTokenizer()\n",
    "        self.forms = Forms()\n",
    "        self.tokens = TokenRanges()\n",
    "\n",
    "\n",
    "    def tokenizeSentences(self, text: str):\n",
    "        self.tokenizer.setText(text)\n",
    "        while self.tokenizer.nextSentence(self.forms, self.tokens):\n",
    "            first = self.tokens[0].start\n",
    "            last = self.tokens[-1].start + self.tokens[-1].length\n",
    "            yield text[first:last]\n",
    "\n",
    "    def tokenizeWords(self, text: str):\n",
    "        self.tokenizer.setText(text)\n",
    "        while self.tokenizer.nextSentence(self.forms, self.tokens):\n",
    "            for form in self.forms:\n",
    "                yield form\n",
    "\n",
    "def detokenize(txt):\n",
    "    # TFIDF to transformer (de)tokenization fixes\n",
    "    txt = txt.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" ?\", \"?\")\n",
    "    txt = txt.replace(\"`` \", '\"').replace(\" ''\", '\"').replace(\" '\", \"'\")\n",
    "    txt = txt.replace(\"-LRB- \", \"(\").replace(\"-RRB-\", \")\")\n",
    "    return txt\n",
    "\n",
    "def detokenize2(txt):\n",
    "    # updated detokenize, most models are not trained with this...\n",
    "    txt = txt.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" ?\", \"?\").replace(\" :\", \":\").replace(\" ;\", \";\")\n",
    "    txt = txt.replace(\"`` \", '\"').replace(\" ''\", '\"').replace(\" '\", \"'\")\n",
    "    txt = txt.replace(\"-LRB- \", \"(\").replace(\"-RRB-\", \")\")\n",
    "    txt = txt.replace(\"( \", \"(\").replace(\" )\", \")\")\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphoDiTa:\n",
    "    def __init__(self, path):\n",
    "        self.tagger = Tagger_load(path)\n",
    "        self.forms = Forms()\n",
    "        self.lemmas = TaggedLemmas()\n",
    "        self.lemmas_forms = TaggedLemmasForms()\n",
    "        self.tokens = TokenRanges()\n",
    "        self.tokenizer = self.tagger.newTokenizer()\n",
    "        self.morpho = self.tagger.getMorpho()\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        self.tokenizer.setText(text)\n",
    "        while self.tokenizer.nextSentence(self.forms, self.tokens):\n",
    "            self.tagger.tag(self.forms, self.lemmas)\n",
    "            for lemma in self.lemmas:\n",
    "                yield (self.morpho.rawLemma(lemma.lemma), self.morpho.rawLemma(lemma.tag))\n",
    "                \n",
    "    def analyze(self, text):\n",
    "        self.tokenizer.setText(text)\n",
    "        while self.tokenizer.nextSentence(self.forms, self.tokens):\n",
    "            for form in self.forms:\n",
    "                result = self.morpho.analyze(form, self.morpho.GUESSER, self.lemmas)\n",
    "                guesser = \"Guesser \" if result == self.morpho.GUESSER else \"\"\n",
    "                for lemma in self.lemmas:\n",
    "                    print(f\"Form: {form};  Guesser {guesser};  Lemma: {lemma.lemma} Tag: {lemma.tag};  Negation: {lemma.tag[10] == 'N'}\")\n",
    "                    \n",
    "    def is_negation(self, word: str) -> bool:\n",
    "        \"\"\"\n",
    "            Returns True if a given word is negation.\n",
    "\n",
    "            Input:\n",
    "                word: a single word\n",
    "        \"\"\"\n",
    "        assert isinstance(word, str), f\"Given word {word} is not a string\"\n",
    "        ret = False\n",
    "        if len(word.strip().split()) == 1:  # single word\n",
    "            result = self.morpho.analyze(word, self.morpho.GUESSER, self.lemmas)\n",
    "            # morphodita can generate more lemmas for a single word (e.g. Nejedlý)\n",
    "            negative = [lemma.tag[10] == 'N' for lemma in self.lemmas]\n",
    "            ret = True if any(negative) else False\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/data/factcheck/ufal/morphodita/czech-morfflex-pdt-161115/czech-morfflex-pdt-161115.tagger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdt = MorphoDiTa(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "form = 'Nejedlý'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdt.is_negation(form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = mdt.morpho.analyze(form, mdt.morpho.GUESSER, mdt.lemmas)\n",
    "guesser = \"Guesser \" if result == mdt.morpho.GUESSER else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Form: Nejedlý;  Guesser ;  Lemma: Nejedlá_;S_^(*1ý) Tag: NNFP1-----A---6;  Negation: False\n",
      "Form: Nejedlý;  Guesser ;  Lemma: Nejedlá_;S_^(*1ý) Tag: NNFP4-----A---6;  Negation: False\n",
      "Form: Nejedlý;  Guesser ;  Lemma: Nejedlá_;S_^(*1ý) Tag: NNFP5-----A---6;  Negation: False\n",
      "Form: Nejedlý;  Guesser ;  Lemma: Nejedlá_;S_^(*1ý) Tag: NNFS2-----A---6;  Negation: False\n",
      "Form: Nejedlý;  Guesser ;  Lemma: Nejedlá_;S_^(*1ý) Tag: NNFS3-----A---6;  Negation: False\n",
      "Form: Nejedlý;  Guesser ;  Lemma: Nejedlá_;S_^(*1ý) Tag: NNFS6-----A---6;  Negation: False\n",
      "Form: Nejedlý;  Guesser ;  Lemma: Nejedlý_;S Tag: NNMP1-----A---6;  Negation: False\n",
      "Form: Nejedlý;  Guesser ;  Lemma: Nejedlý_;S Tag: NNMP4-----A---6;  Negation: False\n",
      "Form: Nejedlý;  Guesser ;  Lemma: Nejedlý_;S Tag: NNMP5-----A---6;  Negation: False\n",
      "Form: Nejedlý;  Guesser ;  Lemma: Nejedlý_;S Tag: NNMS1-----A----;  Negation: False\n",
      "Form: Nejedlý;  Guesser ;  Lemma: Nejedlý_;S Tag: NNMS5-----A----;  Negation: False\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAFP1----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAFP4----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAFP5----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAFS2----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAFS3----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAFS6----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAIP1----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAIP4----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAIP5----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAIS1----1N----;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAIS4----1N----;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAIS5----1N----;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAMP1----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAMP4----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAMP5----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAMS1----1N----;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AAMS5----1N----;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AANP1----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AANP4----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AANP5----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AANS1----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AANS4----1N---6;  Negation: True\n",
      "Form: Nejedlý;  Guesser ;  Lemma: jedlý_^(*4íst) Tag: AANS5----1N---6;  Negation: True\n"
     ]
    }
   ],
   "source": [
    "for lemma in mdt.lemmas:\n",
    "    print(f\"Form: {form};  Guesser {guesser};  Lemma: {lemma.lemma} Tag: {lemma.tag};  Negation: {lemma.tag[10] == 'N'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mdt.lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Petr nešel do obchodu, protože nebylo ještě otevřeno.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Form: Petr;  Guesser ;  Lemma: Petr_;Y Tag: NNMS1-----A----;  Negation: False\n",
      "Form: nešel;  Guesser ;  Lemma: jít Tag: VpYS---XR-NA---;  Negation: True\n",
      "Form: do;  Guesser ;  Lemma: do-1 Tag: RR--2----------;  Negation: False\n",
      "Form: do;  Guesser ;  Lemma: do-7_^(předpona,_sam.) Tag: A2--------A----;  Negation: False\n",
      "Form: obchodu;  Guesser ;  Lemma: obchod Tag: NNIS2-----A----;  Negation: False\n",
      "Form: obchodu;  Guesser ;  Lemma: obchod Tag: NNIS3-----A----;  Negation: False\n",
      "Form: obchodu;  Guesser ;  Lemma: obchod Tag: NNIS6-----A---1;  Negation: False\n",
      "Form: ,;  Guesser ;  Lemma: , Tag: Z:-------------;  Negation: False\n",
      "Form: protože;  Guesser ;  Lemma: protože Tag: J,-------------;  Negation: False\n",
      "Form: nebylo;  Guesser ;  Lemma: být Tag: VpNS---XR-NA---;  Negation: True\n",
      "Form: ještě;  Guesser ;  Lemma: ještě Tag: Db-------------;  Negation: False\n",
      "Form: otevřeno;  Guesser ;  Lemma: otevřít Tag: VsNS---XX-AP---;  Negation: False\n",
      "Form: .;  Guesser ;  Lemma: . Tag: Z:-------------;  Negation: False\n"
     ]
    }
   ],
   "source": [
    "mdt.analyze(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a proportion of negation claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl_files(files: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        files: list containing paths of jsonl files\n",
    "\n",
    "    Return:\n",
    "        dictionary containing merged claims with corresponding labels\n",
    "    \"\"\"\n",
    "    claims, labels = [], []\n",
    "    for file in files:\n",
    "        with open(file) as fr:\n",
    "            for line in fr:\n",
    "                d = json.loads(line)\n",
    "                claims.append(d['claim'])\n",
    "                labels.append(d['label'])\n",
    "    assert len(claims) == len(labels)\n",
    "    # return claims, labels\n",
    "    ret = {'claim': claims, 'label': labels}\n",
    "    return ret\n",
    "\n",
    "\n",
    "def process_jsonl_in_folder(path: str, split: str) -> pd.DataFrame:\n",
    "    \"\"\"Read all jsonl files in given path and process them into a single DataFrame.\"\"\"\n",
    "    if split == 'all':\n",
    "        files = [os.path.join(path, file) for file in os.listdir(path) if file.endswith(\".jsonl\")]\n",
    "    else:\n",
    "        files = [os.path.join(path, split, '.jsonl')]\n",
    "    d = read_jsonl_files(files)\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/data/factcheck/CTK/dataset/splits_concise_ctkId_s0_si0_t0.095_v0.12_source_77'\n",
    "files = [os.path.join(path, file) for file in os.listdir(path) if file.endswith(\".jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/data/factcheck/CTK/dataset/splits_concise_ctkId_s0_si0_t0.095_v0.12_source_77/validation.jsonl',\n",
       " '/mnt/data/factcheck/CTK/dataset/splits_concise_ctkId_s0_si0_t0.095_v0.12_source_77/test.jsonl',\n",
       " '/mnt/data/factcheck/CTK/dataset/splits_concise_ctkId_s0_si0_t0.095_v0.12_source_77/train.jsonl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_jsonl_in_folder(path, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_claims = 0\n",
    "for claim in df.claim:\n",
    "    for w in claim.split():\n",
    "        if mdt.is_negation(w):\n",
    "            neg_claims += 1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative claims: 329 / 3097\n"
     ]
    }
   ],
   "source": [
    "print(f\"Negative claims: {neg_claims} / {len(df.claim)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
